{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Netflix Data Cleaning & Feature Engineering \n",
        "\n",
        "This notebook provides comprehensive data cleaning, validation, and feature engineering for the Netflix dataset analysis pipeline.\n",
        "\n",
        "##  Processing Steps\n",
        "1. **Data Loading & Initial Assessment** - Load raw data and assess quality\n",
        "2. **Data Cleaning & Standardization** - Handle missing values, duplicates, and format issues\n",
        "3. **Temporal Feature Engineering** - Create date-based and temporal features\n",
        "4. **Geographic Feature Engineering** - Process country and regional data\n",
        "5. **Content Feature Engineering** - Create content-specific features\n",
        "6. **Text Feature Engineering** - Process descriptions and text data\n",
        "7. **Derived Analytics Features** - Create business intelligence features\n",
        "8. **Data Quality Validation** - Comprehensive quality checks\n",
        "9. **Final Export** - Save cleaned dataset for downstream analysis\n",
        "\n",
        "##  Output Features\n",
        "- **Temporal**: `date_added_year`, `content_age_when_added`, `is_recent_content`, `decade_released`\n",
        "- **Geographic**: `primary_country`, `continent`, `country_count`, `is_international`\n",
        "- **Content**: `duration_minutes`, `primary_genre`, `genre_count`, `is_adult_content`, `is_family_friendly`\n",
        "- **Analytics**: `is_classic`, `is_short_content`, `is_long_content`, `years_since_release`\n",
        "- **Text**: `description_length`, `description_word_count`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Netflix Data Cleaning & Feature Engineering Pipeline\n",
            "Processing Started: 2025-07-01 08:47:40\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from typing import Dict, List, Optional\n",
        "import logging\n",
        "import random\n",
        "\n",
        "# Set random seed for reproducible description generation\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Add src directory to path\n",
        "sys.path.append('../src')\n",
        "from utils import (connect_db, get_engine, parse_duration, clean_countries, \n",
        "                   clean_genres, get_continent_mapping, print_data_summary, \n",
        "                   setup_plotting_style)\n",
        "\n",
        "# Configure settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "warnings.filterwarnings('ignore')\n",
        "setup_plotting_style()\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('../data/processed', exist_ok=True)\n",
        "os.makedirs('../data/interim', exist_ok=True)\n",
        "os.makedirs('../reports/data_quality', exist_ok=True)\n",
        "\n",
        "print(\"Netflix Data Cleaning & Feature Engineering Pipeline\")\n",
        "print(f\"Processing Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "## 1. Data Loading & Initial Assessment \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. DATA LOADING & INITIAL ASSESSMENT \n",
            "\n",
            "1.1 Loading Raw Dataset\n",
            "Successfully loaded data from: ../data/raw/netflix_raw.csv\n",
            "Raw dataset shape: (8790, 10)\n",
            "\n",
            "1.2 Initial Data Quality Assessment\n",
            "• Dataset dimensions: 8,790 rows × 10 columns\n",
            "• Memory usage: 4.66 MB\n",
            "\n",
            "• Column Overview:\n",
            "   1. show_id              | object       |     0 nulls (  0.0%)\n",
            "   2. type                 | object       |     0 nulls (  0.0%)\n",
            "   3. title                | object       |     0 nulls (  0.0%)\n",
            "   4. director             | object       |     0 nulls (  0.0%)\n",
            "   5. country              | object       |     0 nulls (  0.0%)\n",
            "   6. date_added           | object       |     0 nulls (  0.0%)\n",
            "   7. release_year         | int64        |     0 nulls (  0.0%)\n",
            "   8. rating               | object       |     0 nulls (  0.0%)\n",
            "   9. duration             | object       |     0 nulls (  0.0%)\n",
            "  10. listed_in            | object       |     0 nulls (  0.0%)\n",
            "\n",
            "• Data Sample (first 3 rows):\n",
            "               title    type       country  release_year rating duration\n",
            "Dick Johnson Is Dead   Movie United States          2020  PG-13   90 min\n",
            "           Ganglands TV Show        France          2021  TV-MA 1 Season\n",
            "       Midnight Mass TV Show United States          2021  TV-MA 1 Season\n",
            "\n",
            " Initial assessment saved to: reports/data_quality/01_initial_assessment.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"1. DATA LOADING & INITIAL ASSESSMENT \")\n",
        "\n",
        "\n",
        "# Load raw Netflix data\n",
        "print(\"\\n1.1 Loading Raw Dataset\")\n",
        "\n",
        "\n",
        "try:\n",
        "    # Try loading from different possible locations\n",
        "    possible_paths = [\n",
        "        '../data/raw/netflix_raw.csv',\n",
        "        '../netflix1.csv',\n",
        "        '../data/netflix1.csv'\n",
        "    ]\n",
        "    \n",
        "    df_raw = None\n",
        "    for path in possible_paths:\n",
        "        try:\n",
        "            df_raw = pd.read_csv(path)\n",
        "            data_source = path\n",
        "            print(f\"Successfully loaded data from: {path}\")\n",
        "            break\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "    \n",
        "    if df_raw is None:\n",
        "        raise FileNotFoundError(\"Could not find Netflix dataset in expected locations\")\n",
        "        \n",
        "    print(f\"Raw dataset shape: {df_raw.shape}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\" Error loading data: {e}\")\n",
        "    print(\" Expected file locations:\")\n",
        "    for path in possible_paths:\n",
        "        print(f\"   - {path}\")\n",
        "    raise\n",
        "\n",
        "# Initial data inspection\n",
        "print(f\"\\n1.2 Initial Data Quality Assessment\")\n",
        "\n",
        "print(f\"• Dataset dimensions: {df_raw.shape[0]:,} rows × {df_raw.shape[1]} columns\")\n",
        "print(f\"• Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Column information\n",
        "print(f\"\\n• Column Overview:\")\n",
        "for i, (col, dtype) in enumerate(zip(df_raw.columns, df_raw.dtypes), 1):\n",
        "    null_count = df_raw[col].isnull().sum()\n",
        "    null_pct = (null_count / len(df_raw)) * 100\n",
        "    print(f\"  {i:2d}. {col:20s} | {str(dtype):12s} | {null_count:5d} nulls ({null_pct:5.1f}%)\")\n",
        "\n",
        "# Data sample\n",
        "print(f\"\\n• Data Sample (first 3 rows):\")\n",
        "display_columns = ['title', 'type', 'country', 'release_year', 'rating', 'duration']\n",
        "print(df_raw[display_columns].head(3).to_string(index=False))\n",
        "\n",
        "# Save initial assessment\n",
        "assessment_summary = pd.DataFrame({\n",
        "    'Column': df_raw.columns,\n",
        "    'Data_Type': df_raw.dtypes.astype(str),\n",
        "    'Non_Null_Count': df_raw.count(),\n",
        "    'Null_Count': df_raw.isnull().sum(),\n",
        "    'Null_Percentage': (df_raw.isnull().sum() / len(df_raw) * 100).round(2),\n",
        "    'Unique_Values': [df_raw[col].nunique() for col in df_raw.columns]\n",
        "})\n",
        "\n",
        "assessment_summary.to_csv('../reports/data_quality/01_initial_assessment.csv', index=False)\n",
        "print(f\"\\n Initial assessment saved to: reports/data_quality/01_initial_assessment.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "## 2. Data Cleaning & Standardization \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2. DATA CLEANING & STANDARDIZATION\n",
            "Created working copy with 8,790 records\n",
            "\n",
            "2.1 Missing Value Treatment\n",
            "• No missing values found in dataset\n",
            "\n",
            "2.2 Duplicate Detection & Removal\n",
            "No exact duplicates found\n",
            "Found 3 potential content duplicates (same title, type, year)\n",
            "Kept first occurrence of each duplicate\n",
            "Final dataset after cleaning: 8,787 records\n",
            "\n",
            "2.3 Data Type Standardization\n",
            "Converted release_year to numeric\n",
            "Cleaned whitespace from 9 text columns\n",
            "\n",
            " Data cleaning completed successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"2. DATA CLEANING & STANDARDIZATION\")\n",
        "\n",
        "\n",
        "# Create working copy\n",
        "df_clean = df_raw.copy()\n",
        "print(f\"Created working copy with {len(df_clean):,} records\")\n",
        "\n",
        "# 2.1 Handle Missing Values\n",
        "print(f\"\\n2.1 Missing Value Treatment\")\n",
        "\n",
        "# Identify missing value patterns\n",
        "missing_summary = df_clean.isnull().sum().sort_values(ascending=False)\n",
        "missing_summary = missing_summary[missing_summary > 0]\n",
        "\n",
        "if len(missing_summary) > 0:\n",
        "    print(\"• Missing value counts:\")\n",
        "    for col, count in missing_summary.items():\n",
        "        pct = (count / len(df_clean)) * 100\n",
        "        print(f\"  - {col}: {count:,} ({pct:.1f}%)\")\n",
        "    \n",
        "    # Handle missing values based on column type and business logic\n",
        "    \n",
        "    # Director: Fill with 'Unknown Director' and ensure string type\n",
        "    if 'director' in df_clean.columns:\n",
        "        before_null = df_clean['director'].isnull().sum()\n",
        "        df_clean['director'] = df_clean['director'].astype(str).replace(['nan', 'None'], 'Unknown Director')\n",
        "        df_clean['director'] = df_clean['director'].fillna('Unknown Director')\n",
        "        print(f\"Filled {before_null:,} missing director values with 'Unknown Director'\")\n",
        "    \n",
        "    # Cast: Fill with 'Unknown Cast' and ensure string type\n",
        "    if 'cast' in df_clean.columns:\n",
        "        before_null = df_clean['cast'].isnull().sum()\n",
        "        df_clean['cast'] = df_clean['cast'].astype(str).replace(['nan', 'None'], 'Unknown Cast')\n",
        "        df_clean['cast'] = df_clean['cast'].fillna('Unknown Cast')\n",
        "        print(f\"Filled {before_null:,} missing cast values with 'Unknown Cast'\")\n",
        "    \n",
        "    # Country: Fill with 'Unknown Country'\n",
        "    if 'country' in df_clean.columns:\n",
        "        before_null = df_clean['country'].isnull().sum()\n",
        "        df_clean['country'] = df_clean['country'].fillna('Unknown Country')\n",
        "        print(f\"Filled {before_null:,} missing country values with 'Unknown Country'\")\n",
        "    \n",
        "    # Date added: Remove rows with missing date_added (critical for analysis)\n",
        "    if 'date_added' in df_clean.columns:\n",
        "        before_null = df_clean['date_added'].isnull().sum()\n",
        "        if before_null > 0:\n",
        "            df_clean = df_clean.dropna(subset=['date_added'])\n",
        "            print(f\"Removed {before_null:,} rows with missing date_added\")\n",
        "    \n",
        "    # Rating: Fill with most common rating\n",
        "    if 'rating' in df_clean.columns:\n",
        "        before_null = df_clean['rating'].isnull().sum()\n",
        "        if before_null > 0:\n",
        "            most_common_rating = df_clean['rating'].mode().iloc[0]\n",
        "            df_clean['rating'] = df_clean['rating'].fillna(most_common_rating)\n",
        "            print(f\"Filled {before_null:,} missing rating values with '{most_common_rating}'\")\n",
        "    \n",
        "    # Description: Fill with generic description and ensure string type\n",
        "    if 'description' in df_clean.columns:\n",
        "        before_null = df_clean['description'].isnull().sum()\n",
        "        df_clean['description'] = df_clean['description'].astype(str).replace(['nan', 'None'], 'No description available')\n",
        "        df_clean['description'] = df_clean['description'].fillna('No description available')\n",
        "        print(f\"Filled {before_null:,} missing description values\")\n",
        "        \n",
        "else:\n",
        "    print(\"• No missing values found in dataset\")\n",
        "\n",
        "# 2.2 Remove Duplicates\n",
        "print(f\"\\n2.2 Duplicate Detection & Removal\")\n",
        "\n",
        "# Check for exact duplicates\n",
        "exact_duplicates = df_clean.duplicated().sum()\n",
        "if exact_duplicates > 0:\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "    print(f\"Removed {exact_duplicates:,} exact duplicate rows\")\n",
        "else:\n",
        "    print(\"No exact duplicates found\")\n",
        "\n",
        "# Check for title-based duplicates (same title, type, release year)\n",
        "if all(col in df_clean.columns for col in ['title', 'type', 'release_year']):\n",
        "    title_duplicates = df_clean.duplicated(subset=['title', 'type', 'release_year']).sum()\n",
        "    if title_duplicates > 0:\n",
        "        print(f\"Found {title_duplicates:,} potential content duplicates (same title, type, year)\")\n",
        "        # Keep first occurrence\n",
        "        df_clean = df_clean.drop_duplicates(subset=['title', 'type', 'release_year'], keep='first')\n",
        "        print(f\"Kept first occurrence of each duplicate\")\n",
        "    else:\n",
        "        print(\"No content duplicates found\")\n",
        "\n",
        "print(f\"Final dataset after cleaning: {len(df_clean):,} records\")\n",
        "\n",
        "# 2.3 Data Type Standardization\n",
        "print(f\"\\n2.3 Data Type Standardization\")\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "original_columns = df_clean.columns.tolist()\n",
        "df_clean.columns = df_clean.columns.str.lower().str.replace(' ', '_')\n",
        "renamed_columns = df_clean.columns.tolist()\n",
        "\n",
        "if original_columns != renamed_columns:\n",
        "    print(\"Standardized column names to lowercase with underscores\")\n",
        "    \n",
        "# Ensure proper data types\n",
        "if 'release_year' in df_clean.columns:\n",
        "    df_clean['release_year'] = pd.to_numeric(df_clean['release_year'], errors='coerce')\n",
        "    print(\"Converted release_year to numeric\")\n",
        "\n",
        "# Clean text columns (remove extra whitespace)\n",
        "text_columns = df_clean.select_dtypes(include=['object']).columns\n",
        "for col in text_columns:\n",
        "    df_clean[col] = df_clean[col].astype(str).str.strip()\n",
        "print(f\"Cleaned whitespace from {len(text_columns)} text columns\")\n",
        "\n",
        "print(f\"\\n Data cleaning completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "## 3. Temporal Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3. TEMPORAL FEATURE ENGINEERING\n",
            "\n",
            "3.1 Date Column Processing\n",
            "Extracted year, month, day, weekday, and quarter from date_added\n",
            "Date range: 2008-01-01 to 2021-09-25\n",
            "\n",
            "3.2 Content Age Feature Creation\n",
            "Created content_age_when_added (range: -3 to 93 years)\n",
            "Created is_recent_content flag (5,547 recent titles)\n",
            "Created content_age_category with 6 levels\n",
            "Created content_freshness_score (0-1 scale)\n",
            "\n",
            "3.3 Release Era Feature Creation\n",
            "Created decade_released (10 unique decades)\n",
            "Created release_era categories: {'Streaming_Era': 7455, 'Digital_Era': 807, 'Modern_Era': 274, 'Golden_Age': 199, 'Classic_Era': 52}\n",
            "Created years_since_release and is_classic flag (781 classic titles)\n",
            "\n",
            "3.4 Temporal Trend Features\n",
            "Created netflix_phase categories: {'Global_Phase': 5391, 'Expansion_Phase': 3258, 'Growth_Phase': 117, 'Early_Phase': 21}\n",
            "Created month_name and season_added features\n",
            "\n",
            " Created 7 temporal features successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"3. TEMPORAL FEATURE ENGINEERING\")\n",
        "\n",
        "\n",
        "# 3.1 Date Processing\n",
        "print(f\"\\n3.1 Date Column Processing\")\n",
        "\n",
        "if 'date_added' in df_clean.columns:\n",
        "    # Convert to datetime\n",
        "    df_clean['date_added'] = pd.to_datetime(df_clean['date_added'], errors='coerce')\n",
        "    \n",
        "    # Extract date components\n",
        "    df_clean['date_added_year'] = df_clean['date_added'].dt.year\n",
        "    df_clean['date_added_month'] = df_clean['date_added'].dt.month\n",
        "    df_clean['date_added_day'] = df_clean['date_added'].dt.day\n",
        "    df_clean['date_added_weekday'] = df_clean['date_added'].dt.dayofweek  # 0=Monday\n",
        "    df_clean['date_added_quarter'] = df_clean['date_added'].dt.quarter\n",
        "    \n",
        "    print(f\"Extracted year, month, day, weekday, and quarter from date_added\")\n",
        "    print(f\"Date range: {df_clean['date_added'].min().strftime('%Y-%m-%d')} to {df_clean['date_added'].max().strftime('%Y-%m-%d')}\")\n",
        "else:\n",
        "    print(\"No date_added column found\")\n",
        "\n",
        "# 3.2 Content Age Features\n",
        "print(f\"\\n3.2 Content Age Feature Creation\")\n",
        "\n",
        "if 'release_year' in df_clean.columns and 'date_added_year' in df_clean.columns:\n",
        "    # Content age when added to Netflix\n",
        "    df_clean['content_age_when_added'] = df_clean['date_added_year'] - df_clean['release_year']\n",
        "    \n",
        "    # Recent content flag (added within 2 years of release)\n",
        "    df_clean['is_recent_content'] = df_clean['content_age_when_added'] <= 2\n",
        "    \n",
        "    # Content age categories\n",
        "    df_clean['content_age_category'] = pd.cut(\n",
        "        df_clean['content_age_when_added'], \n",
        "        bins=[-np.inf, 0, 2, 5, 10, 20, np.inf],\n",
        "        labels=['Future_Release', 'Very_Recent', 'Recent', 'Moderate', 'Old', 'Classic']\n",
        "    )\n",
        "    \n",
        "    print(f\"Created content_age_when_added (range: {df_clean['content_age_when_added'].min():.0f} to {df_clean['content_age_when_added'].max():.0f} years)\")\n",
        "    print(f\"Created is_recent_content flag ({df_clean['is_recent_content'].sum():,} recent titles)\")\n",
        "    print(f\"Created content_age_category with 6 levels\")\n",
        "    \n",
        "    # Content freshness score (inverse of age, normalized)\n",
        "    max_age = df_clean['content_age_when_added'].max()\n",
        "    df_clean['content_freshness_score'] = 1 - (df_clean['content_age_when_added'] / max_age)\n",
        "    df_clean['content_freshness_score'] = df_clean['content_freshness_score'].clip(0, 1)\n",
        "    print(f\"Created content_freshness_score (0-1 scale)\")\n",
        "\n",
        "# 3.3 Release Era Features\n",
        "print(f\"\\n3.3 Release Era Feature Creation\")\n",
        "\n",
        "if 'release_year' in df_clean.columns:\n",
        "    # Decade classification\n",
        "    df_clean['decade_released'] = (df_clean['release_year'] // 10) * 10\n",
        "    \n",
        "    # Era classification\n",
        "    def classify_era(year):\n",
        "        if year < 1970:\n",
        "            return 'Classic_Era'\n",
        "        elif year < 1990:\n",
        "            return 'Golden_Age'\n",
        "        elif year < 2000:\n",
        "            return 'Modern_Era'\n",
        "        elif year < 2010:\n",
        "            return 'Digital_Era'\n",
        "        else:\n",
        "            return 'Streaming_Era'\n",
        "    \n",
        "    df_clean['release_era'] = df_clean['release_year'].apply(classify_era)\n",
        "    \n",
        "    # Years since release (from current year)\n",
        "    current_year = datetime.now().year\n",
        "    df_clean['years_since_release'] = current_year - df_clean['release_year']\n",
        "    \n",
        "    # Classic content flag (>20 years old)\n",
        "    df_clean['is_classic'] = df_clean['years_since_release'] > 20\n",
        "    \n",
        "    print(f\"Created decade_released ({df_clean['decade_released'].nunique()} unique decades)\")\n",
        "    print(f\"Created release_era categories: {df_clean['release_era'].value_counts().to_dict()}\")\n",
        "    print(f\"Created years_since_release and is_classic flag ({df_clean['is_classic'].sum():,} classic titles)\")\n",
        "\n",
        "# 3.4 Temporal Trends\n",
        "print(f\"\\n3.4 Temporal Trend Features\")\n",
        "\n",
        "if 'date_added_year' in df_clean.columns:\n",
        "    # Netflix growth phase classification\n",
        "    def classify_netflix_phase(year):\n",
        "        if year < 2013:\n",
        "            return 'Early_Phase'\n",
        "        elif year < 2016:\n",
        "            return 'Growth_Phase'\n",
        "        elif year < 2019:\n",
        "            return 'Expansion_Phase'\n",
        "        else:\n",
        "            return 'Global_Phase'\n",
        "    \n",
        "    df_clean['netflix_phase'] = df_clean['date_added_year'].apply(classify_netflix_phase)\n",
        "    \n",
        "    # Month seasonality features\n",
        "    month_names = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n",
        "                   7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
        "    df_clean['month_name'] = df_clean['date_added_month'].map(month_names)\n",
        "    \n",
        "    # Season classification\n",
        "    def classify_season(month):\n",
        "        if month in [12, 1, 2]:\n",
        "            return 'Winter'\n",
        "        elif month in [3, 4, 5]:\n",
        "            return 'Spring'\n",
        "        elif month in [6, 7, 8]:\n",
        "            return 'Summer'\n",
        "        else:\n",
        "            return 'Fall'\n",
        "    \n",
        "    df_clean['season_added'] = df_clean['date_added_month'].apply(classify_season)\n",
        "    \n",
        "    print(f\"Created netflix_phase categories: {df_clean['netflix_phase'].value_counts().to_dict()}\")\n",
        "    print(f\"Created month_name and season_added features\")\n",
        "\n",
        "temporal_features = ['date_added_year', 'date_added_month', 'content_age_when_added', \n",
        "                    'is_recent_content', 'decade_released', 'years_since_release', 'is_classic']\n",
        "print(f\"\\n Created {len(temporal_features)} temporal features successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "## 4. Geographic Feature Engineering \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4. GEOGRAPHIC FEATURE ENGINEERING\n",
            "\n",
            "4.1 Country Data Processing\n",
            "  Extracted primary_country (86 unique countries)\n",
            "  Created country_count (range: 1 to 1)\n",
            "  Created is_international flag (0 international productions)\n",
            "\n",
            "4.2 Continent Classification\n",
            "  Mapped countries to continents:\n",
            "    - North America: 3,649 titles (41.5%)\n",
            "    - Asia: 2,694 titles (30.7%)\n",
            "    - Europe: 1,460 titles (16.6%)\n",
            "    - Unknown: 336 titles (3.8%)\n",
            "    - Africa: 281 titles (3.2%)\n",
            "    - South America: 236 titles (2.7%)\n",
            "    - Oceania: 131 titles (1.5%)\n",
            "\n",
            "4.3 Regional Classification Features\n",
            "  Created regional flags:\n",
            "    - Major markets: 5,407 titles\n",
            "    - English-speaking: 4,308 titles\n",
            "    - Asian markets: 1,915 titles\n",
            "    - European markets: 1,300 titles\n",
            "\n",
            "4.4 Geographic Diversity Metrics\n",
            "Created country_popularity_rank (1 to 86)\n",
            "Created country_volume_category: {'Top_Producer': 5641, 'Major_Producer': 1731, 'Medium_Producer': 1335, 'Small_Producer': 80}\n",
            "\n",
            "Created 6 geographic features successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"4. GEOGRAPHIC FEATURE ENGINEERING\")\n",
        "\n",
        "\n",
        "# 4.1 Country Processing\n",
        "print(f\"\\n4.1 Country Data Processing\")\n",
        "\n",
        "if 'country' in df_clean.columns:\n",
        "    # Get primary country (first country listed)\n",
        "    df_clean['primary_country'] = df_clean['country'].str.split(',').str[0].str.strip()\n",
        "    \n",
        "    # Count number of countries per title\n",
        "    df_clean['country_count'] = df_clean['country'].str.split(',').str.len()\n",
        "    df_clean['country_count'] = df_clean['country_count'].fillna(1)  # Single country if no comma\n",
        "    \n",
        "    # International production flag\n",
        "    df_clean['is_international'] = df_clean['country_count'] > 1\n",
        "    \n",
        "    # Clean up country names (handle common variations)\n",
        "    country_mapping = {\n",
        "        'United States': 'United States',\n",
        "        'United Kingdom': 'United Kingdom', \n",
        "        'India': 'India',\n",
        "        'South Korea': 'South Korea',\n",
        "        'Japan': 'Japan',\n",
        "        'Canada': 'Canada',\n",
        "        'France': 'France',\n",
        "        'Germany': 'Germany',\n",
        "        'Spain': 'Spain',\n",
        "        'Italy': 'Italy',\n",
        "        'Australia': 'Australia',\n",
        "        'Brazil': 'Brazil',\n",
        "        'Mexico': 'Mexico',\n",
        "        'Netherlands': 'Netherlands',\n",
        "        'Turkey': 'Turkey'\n",
        "    }\n",
        "    \n",
        "    # Apply country name standardization\n",
        "    df_clean['primary_country_clean'] = df_clean['primary_country'].replace(country_mapping)\n",
        "    \n",
        "    print(f\"  Extracted primary_country ({df_clean['primary_country'].nunique()} unique countries)\")\n",
        "    print(f\"  Created country_count (range: {df_clean['country_count'].min():.0f} to {df_clean['country_count'].max():.0f})\")\n",
        "    print(f\"  Created is_international flag ({df_clean['is_international'].sum():,} international productions)\")\n",
        "\n",
        "# 4.2 Continent Mapping\n",
        "print(f\"\\n4.2 Continent Classification\")\n",
        "\n",
        "# Get continent mapping from utils\n",
        "continent_mapping = get_continent_mapping()\n",
        "\n",
        "# Map countries to continents\n",
        "df_clean['continent'] = df_clean['primary_country'].map(continent_mapping)\n",
        "\n",
        "# Fill unknown continents\n",
        "df_clean['continent'] = df_clean['continent'].fillna('Unknown')\n",
        "\n",
        "continent_counts = df_clean['continent'].value_counts()\n",
        "print(f\"  Mapped countries to continents:\")\n",
        "for continent, count in continent_counts.items():\n",
        "    pct = (count / len(df_clean)) * 100\n",
        "    print(f\"    - {continent}: {count:,} titles ({pct:.1f}%)\")\n",
        "\n",
        "# 4.3 Regional Classifications\n",
        "print(f\"\\n4.3 Regional Classification Features\")\n",
        "\n",
        "# Major content markets\n",
        "major_markets = ['United States', 'India', 'United Kingdom', 'South Korea', 'Japan']\n",
        "df_clean['is_major_market'] = df_clean['primary_country'].isin(major_markets)\n",
        "\n",
        "# English-speaking countries\n",
        "english_countries = ['United States', 'United Kingdom', 'Canada', 'Australia', 'Ireland', 'New Zealand']\n",
        "df_clean['is_english_speaking'] = df_clean['primary_country'].isin(english_countries)\n",
        "\n",
        "# Asian markets\n",
        "asian_countries = ['India', 'South Korea', 'Japan', 'China', 'Thailand', 'Philippines', 'Indonesia', 'Malaysia', 'Singapore']\n",
        "df_clean['is_asian_market'] = df_clean['primary_country'].isin(asian_countries)\n",
        "\n",
        "# European markets\n",
        "european_countries = ['United Kingdom', 'France', 'Germany', 'Spain', 'Italy', 'Netherlands', 'Sweden', 'Norway', 'Denmark']\n",
        "df_clean['is_european_market'] = df_clean['primary_country'].isin(european_countries)\n",
        "\n",
        "print(f\"  Created regional flags:\")\n",
        "print(f\"    - Major markets: {df_clean['is_major_market'].sum():,} titles\")\n",
        "print(f\"    - English-speaking: {df_clean['is_english_speaking'].sum():,} titles\")\n",
        "print(f\"    - Asian markets: {df_clean['is_asian_market'].sum():,} titles\")\n",
        "print(f\"    - European markets: {df_clean['is_european_market'].sum():,} titles\")\n",
        "\n",
        "# 4.4 Geographic Diversity Features\n",
        "print(f\"\\n4.4 Geographic Diversity Metrics\")\n",
        "\n",
        "\n",
        "# Country popularity rank\n",
        "country_popularity = df_clean['primary_country'].value_counts()\n",
        "df_clean['country_popularity_rank'] = df_clean['primary_country'].map(\n",
        "    {country: rank for rank, country in enumerate(country_popularity.index, 1)}\n",
        ")\n",
        "\n",
        "# Country production volume category\n",
        "def classify_country_volume(rank):\n",
        "    if rank <= 5:\n",
        "        return 'Top_Producer'\n",
        "    elif rank <= 15:\n",
        "        return 'Major_Producer'\n",
        "    elif rank <= 50:\n",
        "        return 'Medium_Producer'\n",
        "    else:\n",
        "        return 'Small_Producer'\n",
        "\n",
        "df_clean['country_volume_category'] = df_clean['country_popularity_rank'].apply(classify_country_volume)\n",
        "\n",
        "print(f\"Created country_popularity_rank (1 to {df_clean['country_popularity_rank'].max()})\")\n",
        "print(f\"Created country_volume_category: {df_clean['country_volume_category'].value_counts().to_dict()}\")\n",
        "\n",
        "geographic_features = ['primary_country', 'continent', 'country_count', 'is_international', \n",
        "                      'is_major_market', 'is_english_speaking']\n",
        "print(f\"\\nCreated {len(geographic_features)} geographic features successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "## 5. Content Feature Engineering \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5. CONTENT FEATURE ENGINEERING \n",
            "\n",
            "5.1 Duration Feature Processing\n",
            "  Parsed duration_minutes (range: 3 to 312)\n",
            "  Created movie duration categories: {'Standard_Length': 3091, 'Not_Movie': 2663, 'Standard_Short': 1380, 'Long_Film': 934, 'Short_Film': 457, 'Epic_Length': 262}\n",
            "  Created short/long content flags\n",
            "\n",
            "5.2 Genre Feature Processing\n",
            "  Processed 513 unique genre combinations\n",
            "  Created genre_count (range: 1 to 3)\n",
            "  Extracted primary_genre (36 unique)\n",
            "  Created genre_category: {'Other': 4067, 'Drama_Documentary': 1665, 'Action_Thriller': 1362, 'Family_Kids': 990, 'Comedy_Romance': 403, 'Horror_SciFi': 300}\n",
            "\n",
            "5.3 Content Rating Processing\n",
            "  Created content rating flags:\n",
            "    - Adult content: 4,085 titles\n",
            "    - Family-friendly: 1,193 titles\n",
            "    - Teen content: 2,645 titles\n",
            "  Created rating_category: {'Adult': 4003, 'Teen': 2645, 'Unrated': 943, 'Family': 626, 'All_Ages': 567, 'Restricted': 3}\n",
            "\n",
            "5.4 Cast and Director Features\n",
            "  Created director_count (range: 1 to 13)\n",
            "  Has known director: 8,787 titles\n",
            "\n",
            "Created 7 content features successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"5. CONTENT FEATURE ENGINEERING \")\n",
        "\n",
        "\n",
        "# 5.1 Duration Processing\n",
        "print(f\"\\n5.1 Duration Feature Processing\")\n",
        "\n",
        "\n",
        "if 'duration' in df_clean.columns:\n",
        "    # Parse duration using utility function\n",
        "    df_clean['duration_minutes'] = df_clean['duration'].apply(parse_duration)\n",
        "    \n",
        "    # Content type flag\n",
        "    df_clean['is_movie'] = df_clean['type'] == 'Movie'\n",
        "    \n",
        "    # Duration categories for movies\n",
        "    df_clean['is_short_content'] = (df_clean['is_movie']) & (df_clean['duration_minutes'] < 90)\n",
        "    df_clean['is_long_content'] = (df_clean['is_movie']) & (df_clean['duration_minutes'] > 150)\n",
        "    \n",
        "    # Movie duration categories\n",
        "    def classify_movie_duration(row):\n",
        "        if row['type'] != 'Movie':\n",
        "            return 'Not_Movie'\n",
        "        elif row['duration_minutes'] < 60:\n",
        "            return 'Short_Film'\n",
        "        elif row['duration_minutes'] < 90:\n",
        "            return 'Standard_Short'\n",
        "        elif row['duration_minutes'] < 120:\n",
        "            return 'Standard_Length'\n",
        "        elif row['duration_minutes'] < 150:\n",
        "            return 'Long_Film'\n",
        "        else:\n",
        "            return 'Epic_Length'\n",
        "    \n",
        "    df_clean['movie_duration_category'] = df_clean.apply(classify_movie_duration, axis=1)\n",
        "    \n",
        "    # TV show season analysis\n",
        "    tv_shows = df_clean['type'] == 'TV Show'\n",
        "    df_clean['tv_seasons'] = np.where(tv_shows, df_clean['duration_minutes'] / 10, np.nan)  # Convert back to seasons\n",
        "    \n",
        "    print(f\"  Parsed duration_minutes (range: {df_clean['duration_minutes'].min():.0f} to {df_clean['duration_minutes'].max():.0f})\")\n",
        "    print(f\"  Created movie duration categories: {df_clean['movie_duration_category'].value_counts().to_dict()}\")\n",
        "    print(f\"  Created short/long content flags\")\n",
        "\n",
        "# 5.2 Genre Processing\n",
        "print(f\"\\n5.2 Genre Feature Processing\")\n",
        "\n",
        "if 'listed_in' in df_clean.columns:\n",
        "    # Use utility function to clean genres\n",
        "    df_clean['genres_list'] = df_clean['listed_in'].apply(clean_genres)\n",
        "    \n",
        "    # Count number of genres\n",
        "    df_clean['genre_count'] = df_clean['genres_list'].str.len()\n",
        "    \n",
        "    # Extract primary genre (first listed)\n",
        "    df_clean['primary_genre'] = df_clean['genres_list'].str[0]\n",
        "    \n",
        "    # Genre diversity flag\n",
        "    df_clean['is_multi_genre'] = df_clean['genre_count'] > 1\n",
        "    \n",
        "    # Popular genre flags\n",
        "    top_genres = df_clean['primary_genre'].value_counts().head(10).index\n",
        "    for genre in top_genres:\n",
        "        genre_clean = genre.replace(' ', '_').replace('-', '_').lower()\n",
        "        df_clean[f'is_{genre_clean}'] = df_clean['primary_genre'] == genre\n",
        "    \n",
        "    # Genre categories\n",
        "    def classify_genre_type(genre):\n",
        "        if pd.isna(genre):\n",
        "            return 'Unknown'\n",
        "        \n",
        "        genre_lower = str(genre).lower()\n",
        "        \n",
        "        if any(word in genre_lower for word in ['action', 'thriller', 'crime']):\n",
        "            return 'Action_Thriller'\n",
        "        elif any(word in genre_lower for word in ['comedy', 'romantic']):\n",
        "            return 'Comedy_Romance'\n",
        "        elif any(word in genre_lower for word in ['drama', 'documentary']):\n",
        "            return 'Drama_Documentary'\n",
        "        elif any(word in genre_lower for word in ['horror', 'sci-fi', 'fantasy']):\n",
        "            return 'Horror_SciFi'\n",
        "        elif any(word in genre_lower for word in ['children', 'family', 'kids']):\n",
        "            return 'Family_Kids'\n",
        "        else:\n",
        "            return 'Other'\n",
        "    \n",
        "    df_clean['genre_category'] = df_clean['primary_genre'].apply(classify_genre_type)\n",
        "    \n",
        "    print(f\"  Processed {df_clean['listed_in'].nunique()} unique genre combinations\")\n",
        "    print(f\"  Created genre_count (range: {df_clean['genre_count'].min():.0f} to {df_clean['genre_count'].max():.0f})\")\n",
        "    print(f\"  Extracted primary_genre ({df_clean['primary_genre'].nunique()} unique)\")\n",
        "    print(f\"  Created genre_category: {df_clean['genre_category'].value_counts().to_dict()}\")\n",
        "\n",
        "# 5.3 Rating Processing\n",
        "print(f\"\\n5.3 Content Rating Processing\")\n",
        "\n",
        "if 'rating' in df_clean.columns:\n",
        "    # Adult content classification\n",
        "    adult_ratings = ['R', 'NC-17', 'TV-MA', 'NR']  # Added NR as potentially adult\n",
        "    df_clean['is_adult_content'] = df_clean['rating'].isin(adult_ratings)\n",
        "    \n",
        "    # Family-friendly classification\n",
        "    family_ratings = ['G', 'PG', 'TV-G', 'TV-Y', 'TV-Y7', 'TV-Y7-FV']\n",
        "    df_clean['is_family_friendly'] = df_clean['rating'].isin(family_ratings)\n",
        "    \n",
        "    # Teen content\n",
        "    teen_ratings = ['PG-13', 'TV-14']\n",
        "    df_clean['is_teen_content'] = df_clean['rating'].isin(teen_ratings)\n",
        "    \n",
        "    # Rating strictness score (higher = more restrictive)\n",
        "    rating_strictness = {\n",
        "        'G': 1, 'TV-G': 1, 'TV-Y': 1,\n",
        "        'PG': 2, 'TV-Y7': 2, 'TV-Y7-FV': 2,\n",
        "        'PG-13': 3, 'TV-14': 3,\n",
        "        'R': 4, 'TV-MA': 4,\n",
        "        'NC-17': 5, 'NR': 3  # NR assumed moderate\n",
        "    }\n",
        "    df_clean['rating_strictness_score'] = df_clean['rating'].map(rating_strictness).fillna(3)\n",
        "    \n",
        "    # Rating category\n",
        "    def classify_rating_category(rating):\n",
        "        if rating in ['G', 'TV-G', 'TV-Y']:\n",
        "            return 'All_Ages'\n",
        "        elif rating in ['PG', 'TV-Y7', 'TV-Y7-FV']:\n",
        "            return 'Family'\n",
        "        elif rating in ['PG-13', 'TV-14']:\n",
        "            return 'Teen'\n",
        "        elif rating in ['R', 'TV-MA']:\n",
        "            return 'Adult'\n",
        "        elif rating in ['NC-17']:\n",
        "            return 'Restricted'\n",
        "        else:\n",
        "            return 'Unrated'\n",
        "    \n",
        "    df_clean['rating_category'] = df_clean['rating'].apply(classify_rating_category)\n",
        "    \n",
        "    print(f\"  Created content rating flags:\")\n",
        "    print(f\"    - Adult content: {df_clean['is_adult_content'].sum():,} titles\")\n",
        "    print(f\"    - Family-friendly: {df_clean['is_family_friendly'].sum():,} titles\")\n",
        "    print(f\"    - Teen content: {df_clean['is_teen_content'].sum():,} titles\")\n",
        "    print(f\"  Created rating_category: {df_clean['rating_category'].value_counts().to_dict()}\")\n",
        "\n",
        "# 5.4 Cast and Director Features\n",
        "print(f\"\\n5.4 Cast and Director Features\")\n",
        "\n",
        "if 'director' in df_clean.columns:\n",
        "    # Director count\n",
        "    df_clean['director_count'] = df_clean['director'].str.split(',').str.len()\n",
        "    df_clean['director_count'] = df_clean['director_count'].fillna(1)\n",
        "    \n",
        "    # Has known director\n",
        "    df_clean['has_known_director'] = ~df_clean['director'].str.contains('Unknown', na=True)\n",
        "    \n",
        "    print(f\"  Created director_count (range: {df_clean['director_count'].min():.0f} to {df_clean['director_count'].max():.0f})\")\n",
        "    print(f\"  Has known director: {df_clean['has_known_director'].sum():,} titles\")\n",
        "\n",
        "if 'cast' in df_clean.columns:\n",
        "    # Cast count\n",
        "    df_clean['cast_count'] = df_clean['cast'].str.split(',').str.len()\n",
        "    df_clean['cast_count'] = df_clean['cast_count'].fillna(0)\n",
        "    \n",
        "    # Has known cast\n",
        "    df_clean['has_known_cast'] = ~df_clean['cast'].str.contains('Unknown', na=True)\n",
        "    \n",
        "    # Large cast flag\n",
        "    df_clean['has_large_cast'] = df_clean['cast_count'] > 10\n",
        "    \n",
        "    print(f\"  Created cast_count (range: {df_clean['cast_count'].min():.0f} to {df_clean['cast_count'].max():.0f})\")\n",
        "    print(f\"  Has known cast: {df_clean['has_known_cast'].sum():,} titles\")\n",
        "    print(f\"  Large cast (>10): {df_clean['has_large_cast'].sum():,} titles\")\n",
        "\n",
        "content_features = ['duration_minutes', 'is_movie', 'primary_genre', 'genre_count', \n",
        "                   'is_adult_content', 'is_family_friendly', 'rating_category']\n",
        "print(f\"\\nCreated {len(content_features)} content features successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "## 6. Text Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6. TEXT FEATURE ENGINEERING \n",
            "\n",
            "6.1 Text Data Assessment\n",
            "  • Found text-related columns: None\n",
            "  • No existing text data found - will generate descriptions\n",
            "\n",
            "6.2 Description Generation\n",
            "  • Description column not found - generating descriptions from metadata\n",
            "  ✓ Generated descriptions for 8,787 titles\n",
            "   Sample descriptions:\n",
            "    1. A documentaries movie from United States, released in 2020, recommended for teens and adults, direct...\n",
            "    2. This action-packed a crime tv shows tv show from france, released in 2021, for mature audiences, spa...\n",
            "    3. This compelling a tv dramas tv show from united states, released in 2021, for mature audiences, span...\n",
            "\n",
            "6.3 Text Feature Quality Summary\n",
            "   Description Statistics:\n",
            "    • Total descriptions: 8,787\n",
            "    • Average length: 123 characters\n",
            "    • Average words: 20 words\n",
            "    • Has meaningful description: 8,787 (100.0%)\n",
            "\n",
            "   Description Length Categories:\n",
            "    • Medium: 8,359 (95.1%)\n",
            "    • Long: 428 (4.9%)\n",
            "\n",
            "   Sample Generated Descriptions:\n",
            "    1. Your Excellency: A comedies movie from Not Given, released in 2019, suitable for all ages, directed by Funke Akindele.\n",
            "    2. Paradise Lost: This powerful a dramas movie from brazil, released in 2018, for mature audiences, directed by monique gardenberg.\n",
            "    3. Hop: A children & family movies movie from United States, released in 2011, suitable for all ages, directed by Tim Hill.\n",
            "\n",
            "Created 5 text features successfully!\n",
            "Description column ready for text mining analysis!\n"
          ]
        }
      ],
      "source": [
        "print(\"6. TEXT FEATURE ENGINEERING \")\n",
        "\n",
        "\n",
        "# 6.1 Check Existing Text Data\n",
        "print(f\"\\n6.1 Text Data Assessment\")\n",
        "\n",
        "# Check what text columns exist\n",
        "text_related_columns = [col for col in df_clean.columns if any(word in col.lower() for word in ['description', 'summary', 'plot', 'overview'])]\n",
        "print(f\"  • Found text-related columns: {text_related_columns if text_related_columns else 'None'}\")\n",
        "\n",
        "if text_related_columns:\n",
        "    print(f\"  • Using existing text data for analysis\")\n",
        "else:\n",
        "    print(f\"  • No existing text data found - will generate descriptions\")\n",
        "\n",
        "# 6.2 Create Description Column if Missing\n",
        "print(f\"\\n6.2 Description Generation\")\n",
        "\n",
        "if 'description' not in df_clean.columns:\n",
        "    print(\"  • Description column not found - generating descriptions from metadata\")\n",
        "    \n",
        "    def generate_description(row):\n",
        "        \"\"\"Generate a description based on available metadata\"\"\"\n",
        "        desc_parts = []\n",
        "        \n",
        "        # Start with content type and genre\n",
        "        if pd.notna(row.get('type')) and pd.notna(row.get('listed_in')):\n",
        "            content_type = str(row['type']).lower()\n",
        "            genre = str(row['listed_in']).split(',')[0].strip()\n",
        "            desc_parts.append(f\"A {genre.lower()} {content_type}\")\n",
        "        \n",
        "        # Add country information\n",
        "        if pd.notna(row.get('country')):\n",
        "            country = str(row['country']).split(',')[0].strip()\n",
        "            if country != 'Unknown Country':\n",
        "                desc_parts.append(f\"from {country}\")\n",
        "        \n",
        "        # Add release year\n",
        "        if pd.notna(row.get('release_year')):\n",
        "            year = int(row['release_year'])\n",
        "            desc_parts.append(f\"released in {year}\")\n",
        "        \n",
        "        # Add rating information\n",
        "        if pd.notna(row.get('rating')):\n",
        "            rating = str(row['rating'])\n",
        "            if rating in ['G', 'PG', 'TV-Y', 'TV-G']:\n",
        "                desc_parts.append(\"suitable for all ages\")\n",
        "            elif rating in ['PG-13', 'TV-14']:\n",
        "                desc_parts.append(\"recommended for teens and adults\")\n",
        "            elif rating in ['R', 'TV-MA', 'NC-17']:\n",
        "                desc_parts.append(\"for mature audiences\")\n",
        "        \n",
        "        # Add duration information\n",
        "        if pd.notna(row.get('duration')):\n",
        "            duration = str(row['duration'])\n",
        "            if 'min' in duration:\n",
        "                minutes = int(re.findall(r'\\d+', duration)[0])\n",
        "                if minutes < 90:\n",
        "                    desc_parts.append(\"a short film\")\n",
        "                elif minutes > 150:\n",
        "                    desc_parts.append(\"an epic length feature\")\n",
        "            elif 'Season' in duration:\n",
        "                seasons = re.findall(r'\\d+', duration)[0]\n",
        "                desc_parts.append(f\"spanning {seasons} season{'s' if int(seasons) > 1 else ''}\")\n",
        "        \n",
        "        # Add director information if available\n",
        "        if pd.notna(row.get('director')) and str(row['director']) != 'Unknown Director':\n",
        "            director = str(row['director']).split(',')[0].strip()\n",
        "            desc_parts.append(f\"directed by {director}\")\n",
        "        \n",
        "        # Add cast information if available\n",
        "        if pd.notna(row.get('cast')) and str(row['cast']) != 'Unknown Cast':\n",
        "            cast_list = str(row['cast']).split(',')\n",
        "            if len(cast_list) >= 2:\n",
        "                main_cast = ', '.join([cast.strip() for cast in cast_list[:2]])\n",
        "                desc_parts.append(f\"starring {main_cast}\")\n",
        "            elif len(cast_list) == 1:\n",
        "                desc_parts.append(f\"starring {cast_list[0].strip()}\")\n",
        "        \n",
        "        # Create final description\n",
        "        if desc_parts:\n",
        "            description = desc_parts[0].capitalize()\n",
        "            if len(desc_parts) > 1:\n",
        "                description += \" \" + \", \".join(desc_parts[1:])\n",
        "            description += \".\"\n",
        "            \n",
        "            # Add some variety based on genre\n",
        "            genre_keywords = {\n",
        "                'drama': ['compelling', 'emotional', 'powerful'],\n",
        "                'comedy': ['hilarious', 'entertaining', 'funny'],\n",
        "                'action': ['thrilling', 'exciting', 'action-packed'],\n",
        "                'horror': ['scary', 'suspenseful', 'chilling'],\n",
        "                'documentary': ['informative', 'educational', 'eye-opening'],\n",
        "                'romance': ['romantic', 'heartwarming', 'touching'],\n",
        "                'thriller': ['suspenseful', 'gripping', 'intense'],\n",
        "                'sci-fi': ['futuristic', 'imaginative', 'innovative'],\n",
        "                'fantasy': ['magical', 'enchanting', 'mystical']\n",
        "            }\n",
        "            \n",
        "            # Add genre-specific adjective\n",
        "            if pd.notna(row.get('listed_in')):\n",
        "                genre_lower = str(row['listed_in']).lower()\n",
        "                for genre_key, adjectives in genre_keywords.items():\n",
        "                    if genre_key in genre_lower:\n",
        "                        adj = np.random.choice(adjectives)\n",
        "                        description = f\"This {adj} \" + description.lower()\n",
        "                        break\n",
        "            \n",
        "            return description\n",
        "        else:\n",
        "            return f\"A {str(row.get('type', 'content')).lower()} title on Netflix.\"\n",
        "    \n",
        "    # Generate descriptions for all rows\n",
        "    df_clean['description'] = df_clean.apply(generate_description, axis=1)\n",
        "    print(f\"  ✓ Generated descriptions for {len(df_clean):,} titles\")\n",
        "    \n",
        "    # Sample generated descriptions\n",
        "    print(f\"   Sample descriptions:\")\n",
        "    for i, desc in enumerate(df_clean['description'].head(3), 1):\n",
        "        print(f\"    {i}. {desc[:100]}...\")\n",
        "\n",
        "else:\n",
        "    print(\"  • Description column already exists\")\n",
        "\n",
        "# Now create text features\n",
        "df_clean['description_length'] = df_clean['description'].fillna('').astype(str).str.len()\n",
        "df_clean['description_word_count'] = df_clean['description'].fillna('').astype(str).str.split().str.len()\n",
        "df_clean['description_sentence_count'] = df_clean['description'].fillna('').astype(str).str.count(r'[.!?]+')\n",
        "\n",
        "# Text quality flags\n",
        "df_clean['has_description'] = df_clean['description_length'] > 10\n",
        "df_clean['has_long_description'] = df_clean['description_length'] > 100\n",
        "\n",
        "# Description categories\n",
        "def classify_description_length(length):\n",
        "    if length <= 10:\n",
        "        return 'No_Description'\n",
        "    elif length <= 50:\n",
        "        return 'Short'\n",
        "    elif length <= 150:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Long'\n",
        "\n",
        "df_clean['description_length_category'] = df_clean['description_length'].apply(classify_description_length)\n",
        "\n",
        "text_features = ['description', 'description_length', 'description_word_count', 'has_description', 'description_length_category']\n",
        "\n",
        "# 6.3 Text Feature Quality Summary\n",
        "print(f\"\\n6.3 Text Feature Quality Summary\")\n",
        "\n",
        "print(f\"   Description Statistics:\")\n",
        "print(f\"    • Total descriptions: {len(df_clean):,}\")\n",
        "print(f\"    • Average length: {df_clean['description_length'].mean():.0f} characters\")\n",
        "print(f\"    • Average words: {df_clean['description_word_count'].mean():.0f} words\")\n",
        "print(f\"    • Has meaningful description: {df_clean['has_description'].sum():,} ({df_clean['has_description'].mean()*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n   Description Length Categories:\")\n",
        "length_dist = df_clean['description_length_category'].value_counts()\n",
        "for category, count in length_dist.items():\n",
        "    print(f\"    • {category}: {count:,} ({count/len(df_clean)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n   Sample Generated Descriptions:\")\n",
        "sample_descriptions = df_clean['description'].sample(3, random_state=42)\n",
        "for i, (idx, desc) in enumerate(sample_descriptions.items(), 1):\n",
        "    title = df_clean.loc[idx, 'title'] if 'title' in df_clean.columns else f\"Title {idx}\"\n",
        "    print(f\"    {i}. {title}: {desc}\")\n",
        "\n",
        "print(f\"\\nCreated {len(text_features)} text features successfully!\")\n",
        "print(f\"Description column ready for text mining analysis!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "## 7. Derived Analytics Features \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7. DERIVED ANALYTICS FEATURES \n",
            "\n",
            "7.1 Content Popularity Features\n",
            "  Created genre_popularity_rank (1 to 36)\n",
            "  Created country-genre combination features\n",
            "\n",
            "7.2 Content Strategy Features\n",
            "  Netflix originals proxy: 3,175 titles\n",
            "  Created country genre diversity scores\n",
            "\n",
            "Created 3 analytics features successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"7. DERIVED ANALYTICS FEATURES \")\n",
        "\n",
        "\n",
        "# 7.1 Content Popularity Proxies\n",
        "print(f\"\\n7.1 Content Popularity Features\")\n",
        "\n",
        "# Genre popularity rank\n",
        "if 'primary_genre' in df_clean.columns:\n",
        "    genre_popularity = df_clean['primary_genre'].value_counts()\n",
        "    df_clean['genre_popularity_rank'] = df_clean['primary_genre'].map(\n",
        "        {genre: rank for rank, genre in enumerate(genre_popularity.index, 1)}\n",
        "    )\n",
        "    print(f\"  Created genre_popularity_rank (1 to {df_clean['genre_popularity_rank'].max()})\")\n",
        "\n",
        "# Country-genre combination popularity\n",
        "if 'primary_country' in df_clean.columns and 'primary_genre' in df_clean.columns:\n",
        "    df_clean['country_genre_combo'] = df_clean['primary_country'] + \"_\" + df_clean['primary_genre']\n",
        "    combo_popularity = df_clean['country_genre_combo'].value_counts()\n",
        "    df_clean['combo_popularity_rank'] = df_clean['country_genre_combo'].map(\n",
        "        {combo: rank for rank, combo in enumerate(combo_popularity.index, 1)}\n",
        "    )\n",
        "    print(f\"  Created country-genre combination features\")\n",
        "\n",
        "# 7.2 Content Strategy Features\n",
        "print(f\"\\n7.2 Content Strategy Features\")\n",
        "\n",
        "# Netflix originals proxy (recent content from major markets)\n",
        "if all(col in df_clean.columns for col in ['is_recent_content', 'is_major_market']):\n",
        "    df_clean['likely_netflix_original'] = df_clean['is_recent_content'] & df_clean['is_major_market']\n",
        "    print(f\"  Netflix originals proxy: {df_clean['likely_netflix_original'].sum():,} titles\")\n",
        "\n",
        "# Content diversity score per country\n",
        "if 'primary_country' in df_clean.columns:\n",
        "    country_diversity = df_clean.groupby('primary_country')['primary_genre'].nunique()\n",
        "    df_clean['country_genre_diversity'] = df_clean['primary_country'].map(country_diversity)\n",
        "    print(f\"  Created country genre diversity scores\")\n",
        "\n",
        "analytics_features = ['genre_popularity_rank', 'likely_netflix_original', 'country_genre_diversity']\n",
        "print(f\"\\nCreated {len(analytics_features)} analytics features successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "## 8. Data Quality Validation \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8. DATA QUALITY VALIDATION \n",
            "\n",
            "8.1 Feature Quality Validation\n",
            "  • Negative durations: 0\n",
            "  • Future release years: 0\n",
            "  • Impossible content ages: 0\n",
            "\n",
            "8.2 Data Completeness Check\n",
            "  • title: 100.0% complete\n",
            "  • type: 100.0% complete\n",
            "  • primary_country: 100.0% complete\n",
            "  • primary_genre: 100.0% complete\n",
            "  • release_year: 100.0% complete\n",
            "  • date_added_year: 100.0% complete\n",
            "\n",
            "8.3 Statistical Validation\n",
            "  • Numeric features: 23\n",
            "  • Total features created: 77\n",
            "  • Original features: 10\n",
            "  • New features added: 67\n",
            "\n",
            " Data quality validation completed!\n"
          ]
        }
      ],
      "source": [
        "print(\"8. DATA QUALITY VALIDATION \")\n",
        "\n",
        "\n",
        "# 8.1 Feature Validation\n",
        "print(f\"\\n8.1 Feature Quality Validation\")\n",
        "\n",
        "validation_results = {}\n",
        "\n",
        "# Check for negative durations\n",
        "if 'duration_minutes' in df_clean.columns:\n",
        "    negative_durations = (df_clean['duration_minutes'] < 0).sum()\n",
        "    validation_results['negative_durations'] = negative_durations\n",
        "    print(f\"  • Negative durations: {negative_durations}\")\n",
        "\n",
        "# Check for future release years\n",
        "if 'release_year' in df_clean.columns:\n",
        "    future_releases = (df_clean['release_year'] > datetime.now().year).sum()\n",
        "    validation_results['future_releases'] = future_releases\n",
        "    print(f\"  • Future release years: {future_releases}\")\n",
        "\n",
        "# Check for impossible content ages\n",
        "if 'content_age_when_added' in df_clean.columns:\n",
        "    impossible_ages = (df_clean['content_age_when_added'] < -5).sum()  # Allow some tolerance\n",
        "    validation_results['impossible_ages'] = impossible_ages\n",
        "    print(f\"  • Impossible content ages: {impossible_ages}\")\n",
        "\n",
        "# 8.2 Data Completeness Check\n",
        "print(f\"\\n8.2 Data Completeness Check\")\n",
        "\n",
        "key_features = ['title', 'type', 'primary_country', 'primary_genre', 'release_year', 'date_added_year']\n",
        "completeness_report = {}\n",
        "\n",
        "for feature in key_features:\n",
        "    if feature in df_clean.columns:\n",
        "        missing_count = df_clean[feature].isnull().sum()\n",
        "        completeness_pct = ((len(df_clean) - missing_count) / len(df_clean)) * 100\n",
        "        completeness_report[feature] = completeness_pct\n",
        "        print(f\"  • {feature}: {completeness_pct:.1f}% complete\")\n",
        "\n",
        "# 8.3 Statistical Validation\n",
        "print(f\"\\n8.3 Statistical Validation\")\n",
        "\n",
        "numeric_features = df_clean.select_dtypes(include=[np.number]).columns\n",
        "print(f\"  • Numeric features: {len(numeric_features)}\")\n",
        "print(f\"  • Total features created: {len(df_clean.columns)}\")\n",
        "print(f\"  • Original features: {len(df_raw.columns)}\")\n",
        "print(f\"  • New features added: {len(df_clean.columns) - len(df_raw.columns)}\")\n",
        "\n",
        "# Save validation report\n",
        "validation_df = pd.DataFrame([validation_results]).T\n",
        "validation_df.columns = ['Count']\n",
        "validation_df.to_csv('../reports/data_quality/validation_report.csv')\n",
        "\n",
        "completeness_df = pd.DataFrame([completeness_report]).T\n",
        "completeness_df.columns = ['Completeness_Percentage']\n",
        "completeness_df.to_csv('../reports/data_quality/completeness_report.csv')\n",
        "\n",
        "print(f\"\\n Data quality validation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "## 9. Final Export & Summary \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9. FINAL EXPORT & SUMMARY \n",
            "\n",
            "9.1 Feature Engineering Summary\n",
            " Dataset Transformation Summary:\n",
            "  • Original dataset: 8,790 rows × 10 columns\n",
            "  • Final dataset: 8,787 rows × 77 columns\n",
            "  • Data reduction: 3 rows removed\n",
            "  • New features created: 67\n",
            "\n",
            " New Features Created:\n",
            "  Temporal (12): date_added_year, date_added_month, date_added_day...\n",
            "  Geographic (13): primary_country, country_count, is_international...\n",
            "  Content (17): duration_minutes, is_movie, movie_duration_category...\n",
            "  Text (7): description, description_length, description_word_count...\n",
            "  Analytics (6): content_freshness_score, country_popularity_rank, rating_strictness_score...\n",
            "\n",
            "9.2 Export Cleaned Dataset\n",
            " Cleaned dataset exported to: ../data/processed/netflix_cleaned.csv\n",
            " File size: 5.80 MB\n",
            " Feature dictionary saved to: reports/data_quality/feature_dictionary.json\n",
            "  Database export failed: (psycopg2.OperationalError) connection to server a...\n",
            " NETFLIX DATA CLEANING & FEATURE ENGINEERING COMPLETE!\n",
            " Processing Summary:\n",
            "   • Started with: 8,790 records\n",
            "   • Final dataset: 8,787 records (77 features)\n",
            "   • Created: 67 new analytical features\n",
            "   • Data quality: Validated and exported\n",
            "   • Ready for: EDA, ML modeling, and dashboard creation\n",
            " Completed: 2025-07-01 09:01:17\n"
          ]
        }
      ],
      "source": [
        "print(\"9. FINAL EXPORT & SUMMARY \")\n",
        "\n",
        "\n",
        "# 9.1 Feature Summary\n",
        "print(f\"\\n9.1 Feature Engineering Summary\")\n",
        "\n",
        "original_features = set(df_raw.columns)\n",
        "new_features = [col for col in df_clean.columns if col not in original_features]\n",
        "\n",
        "print(f\" Dataset Transformation Summary:\")\n",
        "print(f\"  • Original dataset: {df_raw.shape[0]:,} rows × {df_raw.shape[1]} columns\")\n",
        "print(f\"  • Final dataset: {df_clean.shape[0]:,} rows × {df_clean.shape[1]} columns\")\n",
        "print(f\"  • Data reduction: {df_raw.shape[0] - df_clean.shape[0]:,} rows removed\")\n",
        "print(f\"  • New features created: {len(new_features)}\")\n",
        "\n",
        "print(f\"\\n New Features Created:\")\n",
        "feature_categories = {\n",
        "    'Temporal': [f for f in new_features if any(x in f for x in ['date', 'year', 'age', 'recent', 'classic', 'era', 'decade'])],\n",
        "    'Geographic': [f for f in new_features if any(x in f for x in ['country', 'continent', 'international', 'market'])],\n",
        "    'Content': [f for f in new_features if any(x in f for x in ['duration', 'genre', 'rating', 'adult', 'family', 'movie'])],\n",
        "    'Text': [f for f in new_features if any(x in f for x in ['description', 'text'])],\n",
        "    'Analytics': [f for f in new_features if any(x in f for x in ['rank', 'score', 'popularity', 'diversity'])]\n",
        "}\n",
        "\n",
        "for category, features in feature_categories.items():\n",
        "    if features:\n",
        "        print(f\"  {category} ({len(features)}): {', '.join(features[:3])}{'...' if len(features) > 3 else ''}\")\n",
        "\n",
        "# 9.2 Export Cleaned Dataset\n",
        "print(f\"\\n9.2 Export Cleaned Dataset\")\n",
        "\n",
        "# Export to CSV\n",
        "output_path = '../data/processed/netflix_cleaned.csv'\n",
        "df_clean.to_csv(output_path, index=False)\n",
        "file_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
        "print(f\" Cleaned dataset exported to: {output_path}\")\n",
        "print(f\" File size: {file_size_mb:.2f} MB\")\n",
        "\n",
        "# Export feature dictionary\n",
        "feature_dict = {\n",
        "    'original_features': list(original_features),\n",
        "    'new_features': new_features,\n",
        "    'feature_categories': feature_categories,\n",
        "    'processing_date': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('../reports/data_quality/feature_dictionary.json', 'w') as f:\n",
        "    json.dump(feature_dict, f, indent=2)\n",
        "print(f\" Feature dictionary saved to: reports/data_quality/feature_dictionary.json\")\n",
        "\n",
        "# Optional database export\n",
        "try:\n",
        "    engine = get_engine()\n",
        "    if engine:\n",
        "        df_clean.to_sql('netflix_cleaned', engine, if_exists='replace', index=False)\n",
        "        print(f\"Dataset also exported to PostgreSQL database\")\n",
        "except Exception as e:\n",
        "    print(f\"  Database export failed: {str(e)[:50]}...\")\n",
        "\n",
        "\n",
        "print(f\" NETFLIX DATA CLEANING & FEATURE ENGINEERING COMPLETE!\")\n",
        "\n",
        "print(f\" Processing Summary:\")\n",
        "print(f\"   • Started with: {df_raw.shape[0]:,} records\")\n",
        "print(f\"   • Final dataset: {df_clean.shape[0]:,} records ({len(df_clean.columns)} features)\")\n",
        "print(f\"   • Created: {len(new_features)} new analytical features\")\n",
        "print(f\"   • Data quality: Validated and exported\")\n",
        "print(f\"   • Ready for: EDA, ML modeling, and dashboard creation\")\n",
        "print(f\" Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
